{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33R54QYjCMAJ"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hPcswe0tExci"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
        "llm = ChatOllama(model=\"deepseek-r1:1.5b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZZrs-ijCTYt"
      },
      "source": [
        "## Step 1a - Indexing (Document Ingestion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "1p9AXZycFIH6",
        "outputId": "866503af-45c2-4788-9815-a5204a117109"
      },
      "outputs": [],
      "source": [
        "# video_id = \"Gfr50f6ZBvo\" # only the ID, not full URL\n",
        "# try:\n",
        "#     # If you don’t care which language, this returns the “best” one\n",
        "#     transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en\"])\n",
        "\n",
        "#     # Flatten it to plain text\n",
        "#     transcript = \" \".join(chunk[\"text\"] for chunk in transcript_list)\n",
        "#     print(transcript)\n",
        "\n",
        "# except TranscriptsDisabled:\n",
        "#     print(\"No captions available for this video.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWSK4-VQH8CG",
        "outputId": "01528517-4192-4620-f234-97055e4b6655"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': 'Imagine you happen across a short movie script that',\n",
              "  'start': 1.14,\n",
              "  'duration': 2.836},\n",
              " {'text': 'describes a scene between a person and their AI assistant.',\n",
              "  'start': 3.976,\n",
              "  'duration': 3.164},\n",
              " {'text': \"The script has what the person asks the AI, but the AI's response has been torn off.\",\n",
              "  'start': 7.48,\n",
              "  'duration': 5.58},\n",
              " {'text': 'Suppose you also have this powerful magical machine that can take',\n",
              "  'start': 13.06,\n",
              "  'duration': 3.92},\n",
              " {'text': 'any text and provide a sensible prediction of what word comes next.',\n",
              "  'start': 16.98,\n",
              "  'duration': 3.98},\n",
              " {'text': 'You could then finish the script by feeding in what you have to the machine,',\n",
              "  'start': 21.5,\n",
              "  'duration': 4.006},\n",
              " {'text': \"seeing what it would predict to start the AI's answer,\",\n",
              "  'start': 25.506,\n",
              "  'duration': 2.862},\n",
              " {'text': 'and then repeating this over and over with a growing script completing the dialogue.',\n",
              "  'start': 28.368,\n",
              "  'duration': 4.372},\n",
              " {'text': \"When you interact with a chatbot, this is exactly what's happening.\",\n",
              "  'start': 33.38,\n",
              "  'duration': 3.1},\n",
              " {'text': 'A large language model is a sophisticated mathematical function',\n",
              "  'start': 37.02,\n",
              "  'duration': 3.681},\n",
              " {'text': 'that predicts what word comes next for any piece of text.',\n",
              "  'start': 40.701,\n",
              "  'duration': 3.279},\n",
              " {'text': 'Instead of predicting one word with certainty, though,',\n",
              "  'start': 44.38,\n",
              "  'duration': 3.022},\n",
              " {'text': 'what it does is assign a probability to all possible next words.',\n",
              "  'start': 47.402,\n",
              "  'duration': 3.518},\n",
              " {'text': 'To build a chatbot, you lay out some text that describes an interaction between a user',\n",
              "  'start': 51.62,\n",
              "  'duration': 5.18},\n",
              " {'text': 'and a hypothetical AI assistant, add on whatever the user types in as the first part of',\n",
              "  'start': 56.8,\n",
              "  'duration': 5.24},\n",
              " {'text': 'the interaction, and then have the model repeatedly predict the next word that such a',\n",
              "  'start': 62.04,\n",
              "  'duration': 5.12},\n",
              " {'text': \"hypothetical AI assistant would say in response, and that's what's presented to the user.\",\n",
              "  'start': 67.16,\n",
              "  'duration': 5.3},\n",
              " {'text': 'In doing this, the output tends to look a lot more natural if',\n",
              "  'start': 73.08,\n",
              "  'duration': 3.134},\n",
              " {'text': 'you allow it to select less likely words along the way at random.',\n",
              "  'start': 76.214,\n",
              "  'duration': 3.286},\n",
              " {'text': 'So what this means is even though the model itself is deterministic,',\n",
              "  'start': 80.14,\n",
              "  'duration': 3.48},\n",
              " {'text': \"a given prompt typically gives a different answer each time it's run.\",\n",
              "  'start': 83.62,\n",
              "  'duration': 3.48},\n",
              " {'text': 'Models learn how to make these predictions by processing an enormous amount of text,',\n",
              "  'start': 88.04,\n",
              "  'duration': 4.292},\n",
              " {'text': 'typically pulled from the internet.',\n",
              "  'start': 92.332,\n",
              "  'duration': 1.768},\n",
              " {'text': 'For a standard human to read the amount of text that was used to train GPT-3,',\n",
              "  'start': 94.1,\n",
              "  'duration': 5.371},\n",
              " {'text': 'for example, if they read non-stop 24-7, it would take over 2600 years.',\n",
              "  'start': 99.471,\n",
              "  'duration': 4.889},\n",
              " {'text': 'Larger models since then train on much, much more.',\n",
              "  'start': 104.72,\n",
              "  'duration': 2.62},\n",
              " {'text': 'You can think of training a little bit like tuning the dials on a big machine.',\n",
              "  'start': 108.2,\n",
              "  'duration': 3.58},\n",
              " {'text': 'The way that a language model behaves is entirely determined by these',\n",
              "  'start': 112.28,\n",
              "  'duration': 4.021},\n",
              " {'text': 'many different continuous values, usually called parameters or weights.',\n",
              "  'start': 116.301,\n",
              "  'duration': 4.079},\n",
              " {'text': 'Changing those parameters will change the probabilities',\n",
              "  'start': 121.02,\n",
              "  'duration': 3.079},\n",
              " {'text': 'that the model gives for the next word on a given input.',\n",
              "  'start': 124.099,\n",
              "  'duration': 3.081},\n",
              " {'text': 'What puts the large in large language model is how',\n",
              "  'start': 127.86,\n",
              "  'duration': 2.867},\n",
              " {'text': 'they can have hundreds of billions of these parameters.',\n",
              "  'start': 130.727,\n",
              "  'duration': 3.093},\n",
              " {'text': 'No human ever deliberately sets those parameters.',\n",
              "  'start': 135.2,\n",
              "  'duration': 2.84},\n",
              " {'text': 'Instead, they begin at random, meaning the model just outputs gibberish,',\n",
              "  'start': 138.44,\n",
              "  'duration': 4.203},\n",
              " {'text': \"but they're repeatedly refined based on many example pieces of text.\",\n",
              "  'start': 142.643,\n",
              "  'duration': 3.917},\n",
              " {'text': 'One of these training examples could be just a handful of words,',\n",
              "  'start': 147.14,\n",
              "  'duration': 3.516},\n",
              " {'text': 'or it could be thousands, but in either case, the way this works is to',\n",
              "  'start': 150.656,\n",
              "  'duration': 3.84},\n",
              " {'text': 'pass in all but the last word from that example into the model and',\n",
              "  'start': 154.496,\n",
              "  'duration': 3.624},\n",
              " {'text': 'compare the prediction that it makes with the true last word from the example.',\n",
              "  'start': 158.12,\n",
              "  'duration': 4.22},\n",
              " {'text': 'An algorithm called backpropagation is used to tweak all of the parameters',\n",
              "  'start': 163.26,\n",
              "  'duration': 4.133},\n",
              " {'text': 'in such a way that it makes the model a little more likely to choose',\n",
              "  'start': 167.393,\n",
              "  'duration': 3.803},\n",
              " {'text': 'the true last word and a little less likely to choose all the others.',\n",
              "  'start': 171.196,\n",
              "  'duration': 3.804},\n",
              " {'text': 'When you do this for many, many trillions of examples,',\n",
              "  'start': 175.74,\n",
              "  'duration': 3.01},\n",
              " {'text': 'not only does the model start to give more accurate predictions on the training data,',\n",
              "  'start': 178.75,\n",
              "  'duration': 4.708},\n",
              " {'text': \"but it also starts to make more reasonable predictions on text that it's never\",\n",
              "  'start': 183.458,\n",
              "  'duration': 4.325},\n",
              " {'text': 'seen before.', 'start': 187.783, 'duration': 0.657},\n",
              " {'text': 'Given the huge number of parameters and the enormous amount of training data,',\n",
              "  'start': 189.42,\n",
              "  'duration': 4.499},\n",
              " {'text': 'the scale of computation involved in training a large language model is mind-boggling.',\n",
              "  'start': 193.919,\n",
              "  'duration': 4.961},\n",
              " {'text': 'To illustrate, imagine that you could perform one',\n",
              "  'start': 199.6,\n",
              "  'duration': 2.685},\n",
              " {'text': 'billion additions and multiplications every single second.',\n",
              "  'start': 202.285,\n",
              "  'duration': 3.115},\n",
              " {'text': 'How long do you think it would take for you to do all of the',\n",
              "  'start': 206.06,\n",
              "  'duration': 3.266},\n",
              " {'text': 'operations involved in training the largest language models?',\n",
              "  'start': 209.326,\n",
              "  'duration': 3.214},\n",
              " {'text': 'Do you think it would take a year?',\n",
              "  'start': 213.46,\n",
              "  'duration': 1.579},\n",
              " {'text': 'Maybe something like 10,000 years?',\n",
              "  'start': 216.039,\n",
              "  'duration': 1.921},\n",
              " {'text': 'The answer is actually much more than that.',\n",
              "  'start': 219.02,\n",
              "  'duration': 1.78},\n",
              " {'text': \"It's well over 100 million years.\",\n",
              "  'start': 221.12,\n",
              "  'duration': 2.78},\n",
              " {'text': 'This is only part of the story, though.',\n",
              "  'start': 225.52,\n",
              "  'duration': 1.84},\n",
              " {'text': 'This whole process is called pre-training.',\n",
              "  'start': 227.54,\n",
              "  'duration': 1.68},\n",
              " {'text': 'The goal of auto-completing a random passage of text from the',\n",
              "  'start': 229.5,\n",
              "  'duration': 3.146},\n",
              " {'text': 'internet is very different from the goal of being a good AI assistant.',\n",
              "  'start': 232.646,\n",
              "  'duration': 3.554},\n",
              " {'text': 'To address this, chatbots undergo another type of training,',\n",
              "  'start': 236.88,\n",
              "  'duration': 3.2},\n",
              " {'text': 'just as important, called reinforcement learning with human feedback.',\n",
              "  'start': 240.08,\n",
              "  'duration': 3.68},\n",
              " {'text': 'Workers flag unhelpful or problematic predictions,',\n",
              "  'start': 244.48,\n",
              "  'duration': 3.018},\n",
              " {'text': \"and their corrections further change the model's parameters,\",\n",
              "  'start': 247.498,\n",
              "  'duration': 3.611},\n",
              " {'text': 'making them more likely to give predictions that users prefer.',\n",
              "  'start': 251.109,\n",
              "  'duration': 3.671},\n",
              " {'text': 'Looking back at the pre-training, though, this staggering amount of',\n",
              "  'start': 254.78,\n",
              "  'duration': 4.08},\n",
              " {'text': 'computation is only made possible by using special computer chips that',\n",
              "  'start': 258.86,\n",
              "  'duration': 4.26},\n",
              " {'text': 'are optimized for running many operations in parallel, known as GPUs.',\n",
              "  'start': 263.12,\n",
              "  'duration': 4.14},\n",
              " {'text': 'However, not all language models can be easily parallelized.',\n",
              "  'start': 268.12,\n",
              "  'duration': 3.5},\n",
              " {'text': 'Prior to 2017, most language models would process text one word at a time,',\n",
              "  'start': 272.08,\n",
              "  'duration': 4.737},\n",
              " {'text': 'but then a team of researchers at Google introduced a new model known as the transformer.',\n",
              "  'start': 276.817,\n",
              "  'duration': 5.623},\n",
              " {'text': \"Transformers don't read text from the start to the finish,\",\n",
              "  'start': 283.3,\n",
              "  'duration': 3.445},\n",
              " {'text': 'they soak it all in at once, in parallel.',\n",
              "  'start': 286.745,\n",
              "  'duration': 2.395},\n",
              " {'text': 'The very first step inside a transformer, and most other language models for that matter,',\n",
              "  'start': 289.9,\n",
              "  'duration': 4.7},\n",
              " {'text': 'is to associate each word with a long list of numbers.',\n",
              "  'start': 294.6,\n",
              "  'duration': 2.82},\n",
              " {'text': 'The reason for this is that the training process only works with continuous values,',\n",
              "  'start': 297.86,\n",
              "  'duration': 4.536},\n",
              " {'text': 'so you have to somehow encode language using numbers,',\n",
              "  'start': 302.396,\n",
              "  'duration': 2.916},\n",
              " {'text': 'and each of these lists of numbers may somehow encode the meaning of the',\n",
              "  'start': 305.312,\n",
              "  'duration': 3.942},\n",
              " {'text': 'corresponding word.', 'start': 309.254, 'duration': 1.026},\n",
              " {'text': 'What makes transformers unique is their reliance',\n",
              "  'start': 310.28,\n",
              "  'duration': 3.08},\n",
              " {'text': 'on a special operation known as attention.',\n",
              "  'start': 313.36,\n",
              "  'duration': 2.64},\n",
              " {'text': 'This operation gives all of these lists of numbers a chance to talk to one another',\n",
              "  'start': 316.98,\n",
              "  'duration': 4.704},\n",
              " {'text': 'and refine the meanings they encode based on the context around, all done in parallel.',\n",
              "  'start': 321.684,\n",
              "  'duration': 4.876},\n",
              " {'text': 'For example, the numbers encoding the word bank might be changed based on the',\n",
              "  'start': 327.4,\n",
              "  'duration': 4.307},\n",
              " {'text': 'context surrounding it to somehow encode the more specific notion of a riverbank.',\n",
              "  'start': 331.707,\n",
              "  'duration': 4.473},\n",
              " {'text': 'Transformers typically also include a second type of operation known',\n",
              "  'start': 337.28,\n",
              "  'duration': 3.749},\n",
              " {'text': 'as a feed-forward neural network, and this gives the model extra',\n",
              "  'start': 341.029,\n",
              "  'duration': 3.532},\n",
              " {'text': 'capacity to store more patterns about language learned during training.',\n",
              "  'start': 344.561,\n",
              "  'duration': 3.859},\n",
              " {'text': 'All of this data repeatedly flows through many different iterations of',\n",
              "  'start': 349.28,\n",
              "  'duration': 4.121},\n",
              " {'text': 'these two fundamental operations, and as it does so,',\n",
              "  'start': 353.401,\n",
              "  'duration': 3.077},\n",
              " {'text': 'the hope is that each list of numbers is enriched to encode whatever',\n",
              "  'start': 356.478,\n",
              "  'duration': 4.006},\n",
              " {'text': 'information might be needed to make an accurate prediction of what word',\n",
              "  'start': 360.484,\n",
              "  'duration': 4.18},\n",
              " {'text': 'follows in the passage.', 'start': 364.664, 'duration': 1.336},\n",
              " {'text': 'At the end, one final function is performed on the last vector in this sequence,',\n",
              "  'start': 367.0,\n",
              "  'duration': 4.534},\n",
              " {'text': 'which now has had a chance to be influenced by all the other context from the input text,',\n",
              "  'start': 371.534,\n",
              "  'duration': 5.039},\n",
              " {'text': 'as well as everything the model learned during training,',\n",
              "  'start': 376.573,\n",
              "  'duration': 3.191},\n",
              " {'text': 'to produce a prediction of the next word.',\n",
              "  'start': 379.764,\n",
              "  'duration': 2.296},\n",
              " {'text': \"Again, the model's prediction looks like a probability for every possible next word.\",\n",
              "  'start': 382.48,\n",
              "  'duration': 4.88},\n",
              " {'text': 'Although researchers design the framework for how each of these steps work,',\n",
              "  'start': 388.56,\n",
              "  'duration': 4.234},\n",
              " {'text': \"it's important to understand that the specific behavior is an emergent phenomenon\",\n",
              "  'start': 392.794,\n",
              "  'duration': 4.568},\n",
              " {'text': 'based on how those hundreds of billions of parameters are tuned during training.',\n",
              "  'start': 397.362,\n",
              "  'duration': 4.458},\n",
              " {'text': 'This makes it incredibly challenging to determine',\n",
              "  'start': 402.48,\n",
              "  'duration': 2.59},\n",
              " {'text': 'why the model makes the exact predictions that it does.',\n",
              "  'start': 405.07,\n",
              "  'duration': 2.85},\n",
              " {'text': 'What you can see is that when you use large language model predictions to autocomplete',\n",
              "  'start': 408.44,\n",
              "  'duration': 5.338},\n",
              " {'text': 'a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.',\n",
              "  'start': 413.778,\n",
              "  'duration': 5.462},\n",
              " {'text': \"If you're a new viewer and you're curious about more details on how\",\n",
              "  'start': 425.719,\n",
              "  'duration': 3.107},\n",
              " {'text': 'transformers and attention work, boy do I have some material for you.',\n",
              "  'start': 428.826,\n",
              "  'duration': 3.153},\n",
              " {'text': 'One option is to jump into a series I made about deep learning,',\n",
              "  'start': 432.399,\n",
              "  'duration': 3.681},\n",
              " {'text': 'where we visualize and motivate the details of attention and all the other steps',\n",
              "  'start': 436.08,\n",
              "  'duration': 4.66},\n",
              " {'text': 'in a transformer.', 'start': 440.74, 'duration': 0.979},\n",
              " {'text': 'Also, on my second channel I just posted a talk I gave a couple',\n",
              "  'start': 442.099,\n",
              "  'duration': 3.43},\n",
              " {'text': 'months ago about this topic for the company TNG in Munich.',\n",
              "  'start': 445.529,\n",
              "  'duration': 3.11},\n",
              " {'text': 'Sometimes I actually prefer the content I make as a casual talk rather than a produced',\n",
              "  'start': 449.079,\n",
              "  'duration': 4.022},\n",
              " {'text': 'video, but I leave it up to you which one of these feels like the better follow-on.',\n",
              "  'start': 453.101,\n",
              "  'duration': 3.838}]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcript_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKkcYsaOCrRX"
      },
      "source": [
        "## Step 1b - Indexing (Text Splitting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24i-ZSVXFbnC"
      },
      "outputs": [],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.create_documents([transcript])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dm9sfpQFnF1",
        "outputId": "7b9bea3d-b5a4-47f1-f793-16b6bfdd6a7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "168"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYlrcBrkFO-N",
        "outputId": "b6e33df4-c8a2-4c2d-f929-4af59774bd94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content=\"and and kind of come up with descriptions of the electron clouds where they're gonna go how they're gonna interact when you put two elements together uh and what we try to do is learn a simulation uh uh learner functional that will describe more chemistry types of chemistry so um until now you know you can run expensive simulations but then you can only simulate very small uh molecules very simple molecules we would like to simulate large materials um and so uh today there's no way of doing that and we're building up towards uh building functionals that approximate schrodinger's equation and then allow you to describe uh what the electrons are doing and all materials sort of science and material properties are governed by the electrons and and how they interact so have a good summarization of the simulation through the functional um but one that is still close to what the actual simulation would come out with so what um how difficult is that to ask what's involved in that task is it\")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xYFK7WXC2Ka"
      },
      "source": [
        "## Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYXeS5T7FrC4"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWYkp-NmFSVF",
        "outputId": "36f75b4d-b798-4e06-aeea-5c56c91befe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '7c0f7504-08fa-4f33-8708-d29bfc601f84',\n",
              " 1: '06b75197-142c-4ac0-88c8-1bf9e865763b',\n",
              " 2: '6ff986db-4118-4212-a5fe-39f0cf922677',\n",
              " 3: '447cb65c-cfa2-43aa-ad48-2321366e5762',\n",
              " 4: '30f35db0-6d97-4a6f-b246-27c4cbcb6d13',\n",
              " 5: '3fd10381-afd7-427e-958e-89fc520dc260',\n",
              " 6: '40da6068-0065-4411-9417-11f839de969c',\n",
              " 7: '733cdcc4-7685-40a2-9164-413b8fc4366d',\n",
              " 8: '03adb4d2-e52b-4efd-911d-f7ae4caa4eaa',\n",
              " 9: 'e6997d55-f5b1-4785-bc63-613d57306c68',\n",
              " 10: 'ec55a375-5d7c-46b5-9d0c-73e8f8810712',\n",
              " 11: '2b8a87c8-01a6-4390-a751-543668f921d9',\n",
              " 12: '4a0fa4f7-0abd-4e05-9154-2d90d7f42c9e',\n",
              " 13: 'f36acaa7-947f-4120-8094-5a0a3521ad9a',\n",
              " 14: '87dc4c33-6bcd-4b55-b2ca-1e1d2e30e374',\n",
              " 15: 'bfbfd51c-e879-4c43-8d0f-97cdea1ecf7d',\n",
              " 16: '237bef6d-bc86-49b6-86a3-f300c6cd96de',\n",
              " 17: '62de1f42-4306-40c4-aa52-ab93a2d43754',\n",
              " 18: '22ac2726-8b22-48b7-a457-c9b81799f2a7',\n",
              " 19: '92d2f60c-80d5-4c12-ad2b-cb30a98851e8',\n",
              " 20: '23b0db0d-da2e-4ab9-84db-0ae8c5d15a42',\n",
              " 21: '08bc0031-2f11-468c-a1dd-54449e12db06',\n",
              " 22: 'eb3b8f90-9ee9-44a8-8aee-c5cb784c7eb3',\n",
              " 23: 'c7f6986a-189a-4444-a900-e1e568caee9c',\n",
              " 24: 'e3d7ab3b-7b60-4de0-8a1e-3b8394003fee',\n",
              " 25: '823ab093-dd84-40ba-9c4e-ddab818085b8',\n",
              " 26: 'b4fb574d-9bc3-4c94-9df0-5212275fbb2d',\n",
              " 27: '812a72a5-fdbd-4867-a877-18e807a8d774',\n",
              " 28: '2bcda52b-0e25-4798-bf2a-c8229d70ec57',\n",
              " 29: '4cd08b34-05ce-4f79-b0da-667f71f44936',\n",
              " 30: '57ab4815-bd42-4664-8976-dd4fc53e770d',\n",
              " 31: 'fff29621-f307-44ea-8e14-34d276c124cd',\n",
              " 32: '656a4467-530b-4137-9e60-5a409aa62345',\n",
              " 33: '7da6c567-238a-400b-8ce7-91ee4ef9ccd3',\n",
              " 34: '8234c825-74a1-45ee-a683-b264eb11c29b',\n",
              " 35: '0d860b3c-c53c-451a-b761-b782785cbab4',\n",
              " 36: '19fdaa54-8c4d-4fd4-8b1c-55ab1f3b1fc1',\n",
              " 37: 'd720c8cf-c224-4c15-b494-116bd5d30424',\n",
              " 38: '56ede6f0-d62d-4a42-9b13-d847cf46532e',\n",
              " 39: 'acf86bb1-83b1-4efc-89ef-b0530fc50c18',\n",
              " 40: 'e0ba422e-a794-4ef2-aa51-a75d66e9d9db',\n",
              " 41: '5f46e4dd-022c-40d8-b73c-0901e428b13c',\n",
              " 42: '374e910b-041c-4006-b94b-1a37194ad549',\n",
              " 43: '6b5bc2fc-eb43-428e-8ecb-7874352e1b42',\n",
              " 44: '1add167b-2504-4d51-8974-d0b7644f8c76',\n",
              " 45: '163d291f-437c-4280-af18-7484737e1648',\n",
              " 46: '45bf20b3-7e51-4978-8efa-ad7e4a4b866a',\n",
              " 47: 'ee411984-e0bc-443f-992e-bc5cf3111f3d',\n",
              " 48: '18b51f36-3fff-4b7f-a507-1c9ce1a8982d',\n",
              " 49: 'e0eb5bd3-a9bb-4ceb-97df-85c9dabb5a53',\n",
              " 50: '4c55b502-fce1-4360-8076-42a5db26107a',\n",
              " 51: '5c768812-1807-4678-bdd8-81d1582e2e8c',\n",
              " 52: 'fbd6fcc0-7b6f-4d3c-af02-b2e5ffccbd37',\n",
              " 53: '23807eb3-ed23-4029-bd8d-0db40db80c5d',\n",
              " 54: '9bacf012-cc41-4136-82ed-29a5d4a539a4',\n",
              " 55: 'a3c14979-7346-4d9f-bc39-b8194e4582de',\n",
              " 56: 'ce310fcb-b6f2-4f03-8054-eeafdf2a2bf1',\n",
              " 57: 'a9d46f8a-f7da-4bd0-898c-a13e5439b556',\n",
              " 58: 'fb5e7e98-0bd0-4049-ad0e-a38fb835c2b9',\n",
              " 59: '6fc1a125-73f1-4560-8b0a-e5ba6fd9d403',\n",
              " 60: 'cf11b0a4-d350-4368-bc85-bcf92e275e5c',\n",
              " 61: '3c57fe3f-4c89-48b6-979f-fbca4afdd1d3',\n",
              " 62: '3a6e36df-cf23-4ffe-9d12-49c6b05dc5a2',\n",
              " 63: '07799ca1-a873-4a15-8886-b6fb19d93421',\n",
              " 64: '4b68456d-1e53-4e86-86b2-a8a131a11cb0',\n",
              " 65: 'a29fe24c-d162-41b8-b057-dcf0dc7b987b',\n",
              " 66: '066d9da7-cf6d-4a0c-af6e-5457d9a0675f',\n",
              " 67: '431072f6-d213-4757-98f9-5e4b76b4bec4',\n",
              " 68: '5404b92d-be79-44b1-bf76-bb347bb9dac9',\n",
              " 69: 'ddff1076-f241-45ca-bab9-3e03b1a812e5',\n",
              " 70: '934adbd6-6408-45da-abc5-ebba1785562b',\n",
              " 71: '0ddf7e65-906a-4871-8a0d-f277750dc64c',\n",
              " 72: 'd1e52fd8-c3ae-429b-beab-bcd2e4d26e8b',\n",
              " 73: '20059f0a-9666-45c0-a7b3-2fda7d3b490e',\n",
              " 74: '88610b3f-60af-46c4-9d5b-b76ad086c195',\n",
              " 75: '72b7958a-f509-4348-b49d-ce3ce436afba',\n",
              " 76: 'a3d1ff48-f964-4a66-8b67-d798004f9b54',\n",
              " 77: '24535063-4e37-4196-b8a7-1bdde5382cc6',\n",
              " 78: '798e39ba-4075-4279-904a-d1fc5391bdb1',\n",
              " 79: '81c22ad4-0553-471f-8f02-e4c99c595ec9',\n",
              " 80: '4c2bd757-2bbb-4c72-93f6-b3ea60eb4f91',\n",
              " 81: '46758929-139f-400f-adef-9610f24674a5',\n",
              " 82: '5de22cf4-16d2-4081-a481-709ec357853d',\n",
              " 83: '55806ba1-fbcb-4990-8624-741288e481c7',\n",
              " 84: 'b7e9e8b6-62e5-4ff4-98e0-f64405835a8e',\n",
              " 85: '6f5a3e09-505d-4d93-8385-3b5b5b5934a0',\n",
              " 86: '927c1070-b0e5-412a-acc7-e0228579cfcc',\n",
              " 87: 'dea4fd63-732b-4f24-9d4a-9ce4e91fa641',\n",
              " 88: '99d8bfff-70c8-447d-bbb4-aa02cfbf29f9',\n",
              " 89: '3eb4d7b9-7e8e-47b6-aff4-be9e272b1937',\n",
              " 90: 'b3d00b0f-a8ef-4dc6-aae5-a538ca3355af',\n",
              " 91: 'ba549b8b-25db-4854-b8b8-41952da6fddb',\n",
              " 92: '14f4851b-7f23-46fc-a961-f82640283e5d',\n",
              " 93: 'b2ae8f4e-d4bc-4c3c-a7e0-0f13fbfc2919',\n",
              " 94: 'a1dc5bd4-b901-4d17-b77e-f573d2d20f86',\n",
              " 95: 'fd83b803-2da9-433d-ac21-8961e541214e',\n",
              " 96: '17202855-855f-4792-86c9-c16f8d99648d',\n",
              " 97: '66794e6a-73e9-4d19-b28e-adafa60ae70b',\n",
              " 98: '3c60d0d6-5d01-4dfc-99fc-5c4bb4422cb0',\n",
              " 99: '46a885b0-6a22-49a7-88a2-1582b71eca30',\n",
              " 100: '27b32505-e6ef-4995-9133-9f0eea4f7756',\n",
              " 101: 'e18f33d7-e7a7-4ddb-8591-4e5c7c1d372c',\n",
              " 102: 'a2d388ed-5b21-42b8-a1d2-a1fdf505b9c6',\n",
              " 103: 'a847078b-7f8f-4132-b035-f7c640309c99',\n",
              " 104: '2585c5b6-31ae-4f25-b934-5e7cc42f68bd',\n",
              " 105: 'c0c4a613-b5df-4e18-bb09-a4efd45d5184',\n",
              " 106: '22bf50e1-0946-4ef8-8d5a-0b704d1b7ed6',\n",
              " 107: '90f7e684-e591-4a13-b3bc-5ac1abfc57df',\n",
              " 108: 'e23879ad-4d5d-4418-a8c4-ee57064ab884',\n",
              " 109: '8eddc30e-ff7a-4419-9e07-d055eb056f73',\n",
              " 110: '93c2f66e-1dbc-4e8a-922a-168e8b952852',\n",
              " 111: '61f745b3-6b56-4e5f-8db0-18a8fb7846ee',\n",
              " 112: 'e862f216-444b-4ef3-ae50-2c8bc2480998',\n",
              " 113: '20d73cc0-c41a-4cfe-8235-04f1aad627e5',\n",
              " 114: 'f2f48f74-9482-4128-8dba-d44a057bd2c0',\n",
              " 115: '87dac438-291e-4aa9-ad65-d2dbf3f01297',\n",
              " 116: 'f4b1a30f-ae12-4dcc-9874-9ea203740af2',\n",
              " 117: '0d6e549f-50fe-4d15-bd37-f9475fbafc65',\n",
              " 118: '3f7e1cc1-c2f4-4535-ac49-959571c91f20',\n",
              " 119: '49aabcb4-b26f-49f4-84df-c9837998d954',\n",
              " 120: 'a2371319-6f39-470e-adaf-104ecbe3d7f1',\n",
              " 121: '03f1d880-48f1-43d6-b776-0f15d56f99fa',\n",
              " 122: 'bf7b60c6-bbc6-4d80-a32a-f996025b075b',\n",
              " 123: '5bf9a83d-d4d2-4dd6-ba80-0956d8847b0a',\n",
              " 124: '22748507-a220-4065-b569-5a4fe90e7367',\n",
              " 125: 'c08ae5d4-16c0-4099-8d46-e88899769460',\n",
              " 126: '957cc44d-376f-4f5b-8b57-f2cca92d355c',\n",
              " 127: '7ed9015a-7e17-4c46-bf65-d4560d7031d1',\n",
              " 128: '6b0cd719-8f14-42a0-88fe-467b7c6ba694',\n",
              " 129: '4d4d6af4-0023-42d3-9c1e-0663e32847dc',\n",
              " 130: '1fe7ff26-91f1-4e4c-94df-4a3aa4d3cfce',\n",
              " 131: 'cba11826-b8ae-4258-87e8-6ad9f3761680',\n",
              " 132: '2f2617cd-53b0-4619-ba58-d09300a8889c',\n",
              " 133: '72bc0f75-acc7-4e51-9bd9-2bf7dc51ac73',\n",
              " 134: 'fd0ddd22-a60b-455c-ab78-17775c2dc841',\n",
              " 135: 'aea872f1-ec68-4b11-b091-a604292dbe23',\n",
              " 136: 'c49b75f3-3978-4692-9c44-f3e8a3800bfc',\n",
              " 137: '40aaa374-3d8e-4ef5-9277-1aa673740db7',\n",
              " 138: '92979d26-044c-4454-baaa-ff4ad016703d',\n",
              " 139: 'aa7d5183-fbc3-4112-8fa7-5379f20eb288',\n",
              " 140: 'a714c2eb-4318-444a-a1ed-0096bb018e6a',\n",
              " 141: '59a18bbe-b7c4-4fe9-a1e3-bd123b0e3294',\n",
              " 142: '8c458c0d-113e-4816-99a2-1e31ff26804d',\n",
              " 143: '8e0c045f-4db0-46d7-a95f-daaad108bed0',\n",
              " 144: '3dd998c8-c503-4f44-af5c-9123b6eded0e',\n",
              " 145: '591d8d55-8d0c-4176-a8b0-d12cca88c515',\n",
              " 146: '3f2c5c9e-2fc8-4d7a-8516-9c541b19f58c',\n",
              " 147: '727aed50-04c4-4204-b778-182b15014c8a',\n",
              " 148: '8d688ab9-5c84-4aaa-bc9a-70def267332e',\n",
              " 149: 'e0aee3bf-32a2-4d82-a6b5-687fcb89b897',\n",
              " 150: 'a689d18d-de55-420c-91e1-1720142606c9',\n",
              " 151: 'a6a3fbc6-2087-41df-9853-e6a2ce7d8318',\n",
              " 152: '773efd96-3129-495b-b8ce-453744e2e6bf',\n",
              " 153: '81a018fe-f692-4cdf-ab6b-593961becd3f',\n",
              " 154: '5b681d5e-9e38-4ce4-9fb3-c23fdcedce48',\n",
              " 155: '80a35423-cc8a-4a56-a5a7-fc211c758fa6',\n",
              " 156: 'ab1f6a95-ff8c-4aa8-9046-8e4de91a84c7',\n",
              " 157: 'c0e69797-0677-44fb-9576-5c624b6036fc',\n",
              " 158: '7d14b9ff-7ab2-42eb-b259-aaa839f74181',\n",
              " 159: '306f52e0-b933-458a-b4e5-3e953f4eb30b',\n",
              " 160: '8f8614f3-4d90-4244-8728-79e1eb02e86e',\n",
              " 161: '8452e7ca-6211-4d27-9b21-c23d17396311',\n",
              " 162: '8420569e-0351-488d-af65-1f8445315f2c',\n",
              " 163: '5a65ec2b-b76f-43cb-9a75-48684ece1561',\n",
              " 164: '66d04fbf-e4b5-4d0f-ae94-f40d07c4ca66',\n",
              " 165: '31dca5d1-666b-4aab-af3d-540d1c4f093d',\n",
              " 166: 'ea5a8f69-dd04-408e-8351-d00b299cda65',\n",
              " 167: '2436bdb8-3f5f-49c6-8915-0c654c888700'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.index_to_docstore_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxokTcWEGGAo",
        "outputId": "e05f7a2a-4cb1-4fb0-ba48-e6887877a260"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='2436bdb8-3f5f-49c6-8915-0c654c888700', metadata={}, page_content='demas establish to support this podcast please check out our sponsors in the description and now let me leave you with some words from edskar dykstra computer science is no more about computers than astronomy is about telescopes thank you for listening and hope to see you next time')]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.get_by_ids(['2436bdb8-3f5f-49c6-8915-0c654c888700'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zez1650EDN9J"
      },
      "source": [
        "## Step 2 - Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEuoGUYOF3oG"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcose8VuGFAv",
        "outputId": "f8e5472c-4073-4e44-af3a-f4ffcd023dc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fdba029d2d0>, search_kwargs={'k': 4})"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvrsq08TGGNk",
        "outputId": "55ed9475-4497-4e53-d380-5c4c10bf68cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='7c0f7504-08fa-4f33-8708-d29bfc601f84', metadata={}, page_content=\"the following is a conversation with demus hasabis ceo and co-founder of deepmind a company that has published and builds some of the most incredible artificial intelligence systems in the history of computing including alfred zero that learned all by itself to play the game of gold better than any human in the world and alpha fold two that solved protein folding both tasks considered nearly impossible for a very long time demus is widely considered to be one of the most brilliant and impactful humans in the history of artificial intelligence and science and engineering in general this was truly an honor and a pleasure for me to finally sit down with him for this conversation and i'm sure we will talk many times again in the future this is the lex friedman podcast to support it please check out our sponsors in the description and now dear friends here's demis hassabis let's start with a bit of a personal question am i an ai program you wrote to interview people until i get good enough\"),\n",
              " Document(id='431072f6-d213-4757-98f9-5e4b76b4bec4', metadata={}, page_content='i used to discuss um uh uh what were the sort of founding tenets of deep mind and it was very various things one was um algorithmic advances so deep learning you know jeff hinton and cohen just had just sort of invented that in academia but no one in industry knew about it uh we love reinforcement learning we thought that could be scaled up but also understanding about the human brain had advanced um quite a lot uh in the decade prior with fmri machines and other things so we could get some good hints about architectures and algorithms and and sort of um representations maybe that the brain uses so as at a systems level not at a implementation level um and then the other big things were compute and gpus right so we could see a compute was going to be really useful and it got to a place where it became commoditized mostly through the games industry and and that could be taken advantage of and then the final thing was also mathematical and theoretical definitions of intelligence so'),\n",
              " Document(id='4b68456d-1e53-4e86-86b2-a8a131a11cb0', metadata={}, page_content=\"and how it works this is tough to uh ask you this question because you probably will say it's everything but let's let's try let's try to think to this because you're in a very interesting position where deepmind is the place of some of the most uh brilliant ideas in the history of ai but it's also a place of brilliant engineering so how much of solving intelligence this big goal for deepmind how much of it is science how much is engineering so how much is the algorithms how much is the data how much is the hardware compute infrastructure how much is it the software computer infrastructure yeah um what else is there how much is the human infrastructure and like just the humans interact in certain kinds of ways in all the space of all those ideas how much does maybe like philosophy how much what's the key if um uh if if you were to sort of look back like if we go forward 200 years look back what was the key thing that solved intelligence is that ideas i think it's a combination first\"),\n",
              " Document(id='20059f0a-9666-45c0-a7b3-2fda7d3b490e', metadata={}, page_content=\"ambitious as trying to solve intelligence and you're you're you know it's blue sky research no one knows how to do it you you you need to use any evidence or any source of information you can to help guide you in the right direction or give you confidence you're going in the right direction so so that that was one reason we pushed so hard on that and that's and just going back to your early question about organization the other big thing that i think we innovated with at deepmind to encourage invention and and uh and innovation was the multi-disciplinary organization we built and we still have today so deepmind originally was a confluence of the of the most cutting-edge knowledge in neuroscience with machine learning engineering and mathematics right and and gaming and then since then we built that out even further so we have philosophers here and and uh by you know ethicists but also other types of scientists physicists and so on um and that's what brings together i tried to build a\")]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke('What is deepmind')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8y0wRmoDSVZ"
      },
      "source": [
        "## Step 3 - Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2P2AlJ0GN5L"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-NeLx9wFHzw"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "      You are a helpful assistant.\n",
        "      Answer ONLY from the provided transcript context.\n",
        "      If the context is insufficient, just say you don't know.\n",
        "\n",
        "      {context}\n",
        "      Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = ['context', 'question']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI9BOZQwGizf"
      },
      "outputs": [],
      "source": [
        "question          = \"is the topic of nuclear fusion discussed in this video? if yes then what was discussed\"\n",
        "retrieved_docs    = retriever.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfv8yNFsK_GN",
        "outputId": "79fa2a7e-8d92-45bf-99e4-ad12cd86c7a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='3c60d0d6-5d01-4dfc-99fc-5c4bb4422cb0', metadata={}, page_content=\"so we with this problem and we published it in a nature paper last year uh we held the fusion that we held the plasma in specific shapes so actually it's almost like carving the plasma into different shapes and control and hold it there for the record amount of time so um so that's one of the problems of of fusion sort of um solved so i have a controller that's able to no matter the shape uh contain it continue yeah contain it and hold it in structure and there's different shapes that are better for for the energy productions called droplets and and and so on so um so that was huge and now we're looking we're talking to lots of fusion startups to see what's the next problem we can tackle uh in the fusion area so another fascinating place in a paper title pushing the frontiers of density functionals by solving the fractional electron problem so you're taking on modeling and simulating the quantum mechanical behavior of electrons yes um can you explain this work and can ai model and\"),\n",
              " Document(id='fd83b803-2da9-433d-ac21-8961e541214e', metadata={}, page_content=\"in this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double checked with the team we were going to use carefully and safely i was impressed they managed to persuade them to let us use it and um and it's a it's an amazing test reactor they have there and they try all sorts of pretty crazy experiments on it and um the the the what we tend to look at is if we go into a new domain like fusion what are all the bottleneck problems uh like thinking from first principles you know what are all the bottleneck problems that are still stopping fusion working today and then we look at we you know we get a fusion expert to tell us and then we look at those bottlenecks and we look at the ones which ones are amenable to our ai methods today yes right and and and then and would be interesting from a research perspective from our point of view from an ai point of\"),\n",
              " Document(id='a1dc5bd4-b901-4d17-b77e-f573d2d20f86', metadata={}, page_content='that i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so energy and climate so we talked about disease and biology as being one of the biggest places i think ai can help with i think energy and climate uh is another one so maybe they would be my top two um and fusion is one one area i think ai can help with now fusion has many challenges mostly physics material science and engineering challenges as well to build these massive fusion reactors and contain the plasma and what we try to do whenever we go into a new field to apply our systems is we look for um we talk to domain experts we try and find the best people in the world to collaborate with um in this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double'),\n",
              " Document(id='b2ae8f4e-d4bc-4c3c-a7e0-0f13fbfc2919', metadata={}, page_content=\"like room temperature superconductors or something on my list one day i'd like to like you know have an ai system to help build better optimized batteries all of these sort of mechanical things mr i think a systematic sort of search could be uh guided by a model could be um could be extremely powerful so speaking of which you have a paper on nuclear fusion uh magnetic control of tokamak plasmas to deep reinforcement learning so you uh you're seeking to solve nuclear fusion with deep rl so it's doing control of high temperature plasmas can you explain this work and uh can ai eventually solve nuclear fusion it's been very fun last year or two and very productive because we've been taking off a lot of my dream projects if you like of things that i've collected over the years of areas of science that i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so\")]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "bKwpvAo5G_Pk",
        "outputId": "26f0efd0-b35f-44dd-9735-dfc41c1ab86d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"so we with this problem and we published it in a nature paper last year uh we held the fusion that we held the plasma in specific shapes so actually it's almost like carving the plasma into different shapes and control and hold it there for the record amount of time so um so that's one of the problems of of fusion sort of um solved so i have a controller that's able to no matter the shape uh contain it continue yeah contain it and hold it in structure and there's different shapes that are better for for the energy productions called droplets and and and so on so um so that was huge and now we're looking we're talking to lots of fusion startups to see what's the next problem we can tackle uh in the fusion area so another fascinating place in a paper title pushing the frontiers of density functionals by solving the fractional electron problem so you're taking on modeling and simulating the quantum mechanical behavior of electrons yes um can you explain this work and can ai model and\\n\\nin this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double checked with the team we were going to use carefully and safely i was impressed they managed to persuade them to let us use it and um and it's a it's an amazing test reactor they have there and they try all sorts of pretty crazy experiments on it and um the the the what we tend to look at is if we go into a new domain like fusion what are all the bottleneck problems uh like thinking from first principles you know what are all the bottleneck problems that are still stopping fusion working today and then we look at we you know we get a fusion expert to tell us and then we look at those bottlenecks and we look at the ones which ones are amenable to our ai methods today yes right and and and then and would be interesting from a research perspective from our point of view from an ai point of\\n\\nthat i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so energy and climate so we talked about disease and biology as being one of the biggest places i think ai can help with i think energy and climate uh is another one so maybe they would be my top two um and fusion is one one area i think ai can help with now fusion has many challenges mostly physics material science and engineering challenges as well to build these massive fusion reactors and contain the plasma and what we try to do whenever we go into a new field to apply our systems is we look for um we talk to domain experts we try and find the best people in the world to collaborate with um in this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double\\n\\nlike room temperature superconductors or something on my list one day i'd like to like you know have an ai system to help build better optimized batteries all of these sort of mechanical things mr i think a systematic sort of search could be uh guided by a model could be um could be extremely powerful so speaking of which you have a paper on nuclear fusion uh magnetic control of tokamak plasmas to deep reinforcement learning so you uh you're seeking to solve nuclear fusion with deep rl so it's doing control of high temperature plasmas can you explain this work and uh can ai eventually solve nuclear fusion it's been very fun last year or two and very productive because we've been taking off a lot of my dream projects if you like of things that i've collected over the years of areas of science that i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so\""
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bikWKZWDiqB"
      },
      "outputs": [],
      "source": [
        "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LOFVVAbLYvU",
        "outputId": "b6c47460-fdf3-4dad-ebe5-611b3dfe854b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"\\n      You are a helpful assistant.\\n      Answer ONLY from the provided transcript context.\\n      If the context is insufficient, just say you don't know.\\n\\n      so we with this problem and we published it in a nature paper last year uh we held the fusion that we held the plasma in specific shapes so actually it's almost like carving the plasma into different shapes and control and hold it there for the record amount of time so um so that's one of the problems of of fusion sort of um solved so i have a controller that's able to no matter the shape uh contain it continue yeah contain it and hold it in structure and there's different shapes that are better for for the energy productions called droplets and and and so on so um so that was huge and now we're looking we're talking to lots of fusion startups to see what's the next problem we can tackle uh in the fusion area so another fascinating place in a paper title pushing the frontiers of density functionals by solving the fractional electron problem so you're taking on modeling and simulating the quantum mechanical behavior of electrons yes um can you explain this work and can ai model and\\n\\nin this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double checked with the team we were going to use carefully and safely i was impressed they managed to persuade them to let us use it and um and it's a it's an amazing test reactor they have there and they try all sorts of pretty crazy experiments on it and um the the the what we tend to look at is if we go into a new domain like fusion what are all the bottleneck problems uh like thinking from first principles you know what are all the bottleneck problems that are still stopping fusion working today and then we look at we you know we get a fusion expert to tell us and then we look at those bottlenecks and we look at the ones which ones are amenable to our ai methods today yes right and and and then and would be interesting from a research perspective from our point of view from an ai point of\\n\\nthat i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so energy and climate so we talked about disease and biology as being one of the biggest places i think ai can help with i think energy and climate uh is another one so maybe they would be my top two um and fusion is one one area i think ai can help with now fusion has many challenges mostly physics material science and engineering challenges as well to build these massive fusion reactors and contain the plasma and what we try to do whenever we go into a new field to apply our systems is we look for um we talk to domain experts we try and find the best people in the world to collaborate with um in this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double\\n\\nlike room temperature superconductors or something on my list one day i'd like to like you know have an ai system to help build better optimized batteries all of these sort of mechanical things mr i think a systematic sort of search could be uh guided by a model could be um could be extremely powerful so speaking of which you have a paper on nuclear fusion uh magnetic control of tokamak plasmas to deep reinforcement learning so you uh you're seeking to solve nuclear fusion with deep rl so it's doing control of high temperature plasmas can you explain this work and uh can ai eventually solve nuclear fusion it's been very fun last year or two and very productive because we've been taking off a lot of my dream projects if you like of things that i've collected over the years of areas of science that i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so\\n      Question: is the topic of nuclear fusion discussed in this video? if yes then what was discussed\\n    \")"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxxcV2C_DXqt"
      },
      "source": [
        "## Step 4 - Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX6vxSoUHBok",
        "outputId": "eade1e56-b8af-4b7e-c34b-08a4a1b0da00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, the topic of nuclear fusion is discussed in the video. The discussion includes the following points:\n",
            "\n",
            "1. The speaker mentions a problem in fusion that was published in a Nature paper, where they developed a controller that can hold plasma in specific shapes for a record amount of time, which is crucial for energy production.\n",
            "\n",
            "2. They talk about collaborating with EPFL in Switzerland, which has a test reactor that they used for their experiments. The focus is on identifying bottleneck problems in fusion and applying AI methods to address those challenges.\n",
            "\n",
            "3. The speaker emphasizes the potential of AI to help accelerate solutions in energy and climate, specifically mentioning fusion as an area where AI can contribute.\n",
            "\n",
            "4. They also mention their work on magnetic control of tokamak plasmas using deep reinforcement learning, indicating that they are exploring how AI can assist in controlling high-temperature plasmas for nuclear fusion.\n"
          ]
        }
      ],
      "source": [
        "answer = llm.invoke(final_prompt)\n",
        "print(answer.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH2Ph0NcDlo5"
      },
      "source": [
        "## Building a Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdTwSS3nHKRz"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGezE1qYQJ76"
      },
      "outputs": [],
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmYnYqbWQWLi"
      },
      "outputs": [],
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGI1hEvfQvLb",
        "outputId": "093b395c-69da-44d8-dbef-b313f3752687"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': \"the following is a conversation with demus hasabis ceo and co-founder of deepmind a company that has published and builds some of the most incredible artificial intelligence systems in the history of computing including alfred zero that learned all by itself to play the game of gold better than any human in the world and alpha fold two that solved protein folding both tasks considered nearly impossible for a very long time demus is widely considered to be one of the most brilliant and impactful humans in the history of artificial intelligence and science and engineering in general this was truly an honor and a pleasure for me to finally sit down with him for this conversation and i'm sure we will talk many times again in the future this is the lex friedman podcast to support it please check out our sponsors in the description and now dear friends here's demis hassabis let's start with a bit of a personal question am i an ai program you wrote to interview people until i get good enough\\n\\ndeeper maybe simpler explanation yes of things right than the standard model of physics which we know doesn't work but we still keep adding to so um and and that's how i think the beginning of an explanation would look and it would start encompassing many of the mysteries that we have wondered about for thousands of years like you know consciousness uh life and gravity all of these things yeah giving us a glimpses of explanations for those things yeah well um damas dear one of the special human beings in this giant puzzle of ours and it's a huge honor that you would take a pause from the bigger puzzle to solve this small puzzle of a conversation with me today it's truly an honor and a pleasure thank you thank you i really enjoyed it thanks lex thanks for listening to this conversation with demas establish to support this podcast please check out our sponsors in the description and now let me leave you with some words from edskar dykstra computer science is no more about computers than\\n\\nout our sponsors in the description and now dear friends here's demis hassabis let's start with a bit of a personal question am i an ai program you wrote to interview people until i get good enough to interview you well i'll be impressed if if you were i'd be impressed by myself if you were i don't think we're quite up to that yet but uh maybe you're from the future lex if you did would you tell me is that is that a good thing to tell a language model that's tasked with interviewing that it is in fact um ai maybe we're in a kind of meta turing test uh probably probably it would be a good idea not to tell you so it doesn't change your behavior right this is a kind of heisenberg uncertainty principle situation if i told you you behave differently yeah maybe that's what's happening with us of course this is a benchmark from the future where they replay 2022 as a year before ais were good enough yet and now we want to see is it going to pass exactly if i was such a program would you be\\n\\ndemas establish to support this podcast please check out our sponsors in the description and now let me leave you with some words from edskar dykstra computer science is no more about computers than astronomy is about telescopes thank you for listening and hope to see you next time\",\n",
              " 'question': 'who is Demis'}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_chain.invoke('who is Demis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6osgdBfRCPN"
      },
      "outputs": [],
      "source": [
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3e2en89QyOC"
      },
      "outputs": [],
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Ur7Ph_xlRE-7",
        "outputId": "92122b01-36e9-4de4-85cb-6eb1e893a33d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video features a conversation with Demas, who discusses the need for deeper and simpler explanations in physics, particularly in relation to consciousness, life, and gravity. He emphasizes the limitations of the current standard model of physics and the importance of exploring more fundamental explanations. Additionally, he talks about advancements in fusion research, specifically how they have managed to hold plasma in specific shapes for extended periods, which is a significant step in fusion energy production. The conversation touches on the potential of AI in modeling quantum mechanical behavior, particularly regarding electrons. The discussion concludes with a quote about the nature of computer science.'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "main_chain.invoke('Can you summarize the video')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyERl2UwRKn6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
